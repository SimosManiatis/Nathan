# PPO Training Configuration

env:
  width: 8
  height: 8
  trap_density: [0.03, 0.05] # Stage 2C: No safe maps
  max_steps_multiplier: 4 
  num_keys: 3
  dense_reward: true
  step_cost: 0.02
  timeout_penalty: 10.0
  success_reward: 20.0 # Reverted to standard
  min_traps: 1 # Force at least one trap

training:
  algo: "PPO"
  policy: "MultiInputPolicy"
  total_timesteps: 2000000 # 2M steps needed for tough task
  learning_rate: 0.0001
  n_steps: 4096 # Increased horizon for 8x8 exploration
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01 # Low entropy to reduce trap noise
  vf_coef: 0.5
  max_grad_norm: 0.5
  
  # Checkpointing
  checkpoint_freq: 10000
  log_interval: 10

evaluation:
  eval_freq: 5000
  n_eval_episodes: 20
  benchmark_path: "configs/maps/benchmark_seeds.yaml" # Will be created
